"""
🚀 ENHANCED SPECIALIST AGENT
Combina la stabilità di specialist_minimal con le features avanzate del vecchio specialist
Rispetta tutti i 14 pilastri strategici
"""

import logging
import os
import json
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import time

# CRITICAL: Load environment variables for OpenAI API access
from dotenv import load_dotenv
load_dotenv()

logger = logging.getLogger(__name__)

# Import SDK components
try:
    from agents import (
        Agent as OpenAIAgent, 
        Runner,
        function_tool,
        WebSearchTool,
        FileSearchTool,
        handoff,
        RunContextWrapper,
        RunResult,
        input_guardrail,
        output_guardrail
    )
    SDK_AVAILABLE = True
except ImportError:
    SDK_AVAILABLE = False
    logger.warning("OpenAI Agents SDK not available - using minimal fallback")

from models import Agent as AgentModel, Task, TaskStatus, TaskExecutionOutput, AgentStatus
from pydantic import BaseModel
from typing import Any

# Orchestration Context Model
class OrchestrationContext(BaseModel):
    """Pydantic model for orchestration context with SDK RunContextWrapper"""
    workspace_id: str
    task_id: str
    agent_id: str
    agent_role: str
    agent_seniority: str
    task_name: str
    task_description: str
    execution_metadata: Dict[str, Any] = {}
    available_agents: List[Dict[str, Any]] = []
    orchestration_state: Dict[str, Any] = {}

# Enhanced TaskExecutionOutput with additional fields for compatibility
class EnhancedTaskExecutionOutput(TaskExecutionOutput):
    summary: Optional[str] = None
    structured_content: Optional[str] = None

from database import update_agent_status

# Import orchestrators for pillar compliance
try:
    from services.unified_memory_engine import unified_memory_engine
    MEMORY_AVAILABLE = True
except ImportError:
    MEMORY_AVAILABLE = False
    logger.warning("Memory system not available")

try:
    from ai_quality_assurance.unified_quality_engine import unified_quality_engine
    QUALITY_AVAILABLE = True
except ImportError:
    QUALITY_AVAILABLE = False
    logger.warning("Quality engine not available")


class SpecialistAgent:
    """
    Enhanced SpecialistAgent that combines stability with advanced features
    
    Key improvements:
    1. Asset-oriented output (Pillar 12)
    2. Tool integration (Pillar 14)
    3. Memory system hooks (Pillar 6)
    4. Quality validation (Pillar 8)
    5. Simple, focused prompts (prevent hanging)
    """
    
    def __init__(self, agent_data: AgentModel, all_workspace_agents_data: List[AgentModel] = None):
        self.agent_data = agent_data
        self.all_workspace_agents_data = all_workspace_agents_data or []
        self.tools = self._initialize_tools()
        
        # Initialize guardrails
        if SDK_AVAILABLE:
            self.input_guardrail = self._create_input_guardrail()
            self.output_guardrail = self._create_output_guardrail()
        else:
            self.input_guardrail = None
            self.output_guardrail = None
        
    def _initialize_tools(self) -> List[Any]:
        """Initialize real tools for authentic content (Pillar 14)"""
        tools = []
        
        if SDK_AVAILABLE:
            # Add web search for real-time data
            try:
                tools.append(WebSearchTool())
            except Exception as e:
                logger.warning(f"WebSearchTool not available: {e}")
            
            # FileSearchTool requires vector_store_ids
            # Only add if workspace has vector stores configured
            if hasattr(self.agent_data, 'vector_store_ids') and self.agent_data.vector_store_ids:
                try:
                    tools.append(FileSearchTool(vector_store_ids=self.agent_data.vector_store_ids))
                except Exception as e:
                    logger.warning(f"FileSearchTool not available: {e}")
            
            # Add native SDK handoff tools for each workspace agent
            if self.all_workspace_agents_data:
                tools.extend(self._create_native_handoff_tools())
            
        return tools
    
    def _create_input_guardrail(self):\n        \"\"\"Create input guardrail for task validation\"\"\"\n        @input_guardrail\n        def validate_task_input(task_input: str) -> str:\n            \"\"\"Validate task input for safety and completeness\"\"\"\n            # Check for potentially harmful content\n            harmful_patterns = [\n                \"delete\", \"remove\", \"destroy\", \"malicious\", \"hack\", \"exploit\",\n                \"password\", \"secret\", \"key\", \"token\", \"credentials\"\n            ]\n            \n            task_lower = task_input.lower()\n            for pattern in harmful_patterns:\n                if pattern in task_lower:\n                    raise ValueError(f\"Input contains potentially harmful content: {pattern}\")\n            \n            # Check for minimum content length\n            if len(task_input.strip()) < 10:\n                raise ValueError(\"Task input too short - please provide more details\")\n            \n            # Check for placeholder content\n            placeholder_patterns = [\"todo\", \"placeholder\", \"example\", \"sample\"]\n            for pattern in placeholder_patterns:\n                if pattern in task_lower and len(task_input) < 50:\n                    raise ValueError(f\"Task appears to contain placeholder content: {pattern}\")\n            \n            return task_input\n        \n        return validate_task_input\n    \n    def _create_output_guardrail(self):\n        \"\"\"Create output guardrail for quality validation\"\"\"\n        @output_guardrail\n        def validate_task_output(task_output: str) -> str:\n            \"\"\"Validate task output for quality and completeness\"\"\"\n            # Check for minimum output length\n            if len(task_output.strip()) < 20:\n                raise ValueError(\"Output too short - please provide more detailed results\")\n            \n            # Check for placeholder content\n            placeholder_indicators = [\n                \"lorem ipsum\", \"placeholder\", \"todo\", \"tbd\", \"to be determined\",\n                \"example\", \"sample\", \"dummy\", \"test\", \"[insert\", \"<insert\"\n            ]\n            \n            output_lower = task_output.lower()\n            for indicator in placeholder_indicators:\n                if indicator in output_lower:\n                    raise ValueError(f\"Output contains placeholder content: {indicator}\")\n            \n            # Check for proper completion indicators\n            completion_indicators = [\"completed\", \"finished\", \"done\", \"result\", \"output\", \"analysis\"]\n            has_completion_indicator = any(indicator in output_lower for indicator in completion_indicators)\n            \n            if not has_completion_indicator and len(task_output) < 100:\n                raise ValueError(\"Output lacks clear completion indicators\")\n            \n            return task_output\n        \n        return validate_task_output\n    \n    def _create_native_handoff_tools(self):
        """Create native SDK handoff tools for each workspace agent"""
        handoff_tools = []
        
        for agent_data in self.all_workspace_agents_data:
            # Skip self
            if agent_data.id == self.agent_data.id:
                continue
                
            # Create handoff function for this specific agent
            agent_handoff = handoff(
                name=f"handoff_to_{agent_data.role.lower().replace(' ', '_')}",
                description=f"Hand off task to {agent_data.role} ({agent_data.seniority}) - {agent_data.name}",
                instructions=f"""
                Use this handoff when you need to delegate work to a {agent_data.role} specialist.
                
                Agent Profile:
                - Role: {agent_data.role}
                - Seniority: {agent_data.seniority}
                - Skills: {', '.join(agent_data.skills or [])}
                - Specialization: {agent_data.personality_traits.get('specialization', 'General') if agent_data.personality_traits else 'General'}
                
                When to use:
                - Task requires {agent_data.role} expertise
                - Current approach needs {agent_data.seniority} level guidance
                - Specialized skills needed: {', '.join(agent_data.skills or [])}
                """
            )
            
            handoff_tools.append(agent_handoff)
            
        return handoff_tools
    
    def _create_enhanced_prompt(self, task: Task) -> str:
        """
        Create focused prompt that includes asset formatting
        but stays simple to prevent hanging
        """
        # Detect if this is an asset production task
        is_asset_task = (
            "asset" in task.name.lower() or 
            "create" in task.name.lower() or
            task.context_data and task.context_data.get("asset_production")
        )
        
        base_prompt = f"""
You are a {self.agent_data.seniority} {self.agent_data.role}.
Task: {task.name}
Description: {task.description}

EXECUTION RULES:
1. Complete the task using your expertise and available tools
2. Use WebSearchTool for current data, FileSearchTool for documents
3. Use handoff tools when task requires different expertise or role
4. Provide concrete, actionable output with NO placeholders

HANDOFF GUIDANCE:
- Use handoff when task exceeds your role capabilities
- Provide clear context and specific requirements to target agent
- Include summary of work completed before handoff
- Choose appropriate specialist based on task requirements
"""

        if is_asset_task:
            base_prompt += """
4. Format output using these patterns for better readability:

For Tables: ## TABLE: name
| Column1 | Column2 | Column3 |
|---------|---------|---------|
| Data    | Data    | Data    |
## END_TABLE

For Cards: ## CARD: type
TITLE: Main Title
CONTENT: Details
ACTION: Call to action
## END_CARD

5. Include both structured_content and rendered output
"""

        base_prompt += """
Final output format:
{
    "task_id": "TASK_ID",
    "status": "completed",
    "summary": "What you accomplished",
    "result": "Your concrete output",
    "structured_content": "Optional: formatted content"
}
"""
        
        return base_prompt.strip()
        
    async def execute(self, task: Task, session: Optional[Any] = None) -> TaskExecutionOutput:
        """Execute task with all pillar integrations"""
        logger.info(f"🚀 Enhanced execution for task {task.id}")
        
        try:
            # Update agent status
            await update_agent_status(str(self.agent_data.id), AgentStatus.BUSY.value)
            
            if not SDK_AVAILABLE:
                raise Exception("OpenAI Agents SDK required for enhanced execution")
            
            # Create orchestration context
            orchestration_context = OrchestrationContext(
                workspace_id=str(task.workspace_id),
                task_id=str(task.id),
                agent_id=str(self.agent_data.id),
                agent_role=self.agent_data.role,
                agent_seniority=self.agent_data.seniority,
                task_name=task.name,
                task_description=task.description,
                execution_metadata={
                    "started_at": datetime.now().isoformat(),
                    "agent_name": self.agent_data.name,
                    "model": "gpt-4o-mini"
                },
                available_agents=[
                    {
                        "id": str(agent.id),
                        "name": agent.name,
                        "role": agent.role,
                        "seniority": agent.seniority,
                        "skills": agent.skills
                    }
                    for agent in self.all_workspace_agents_data
                ],
                orchestration_state={"execution_phase": "starting"}
            )
            
            # Create agent with enhanced prompt and guardrails
            agent_config = {
                "name": self.agent_data.name,
                "instructions": self._create_enhanced_prompt(task),
                "model": "gpt-4o-mini",  # Faster model for better performance
                "tools": self.tools
            }
            
            # Add guardrails if available
            if self.input_guardrail:
                agent_config["input_guardrails"] = [self.input_guardrail]
            if self.output_guardrail:
                agent_config["output_guardrails"] = [self.output_guardrail]
            
            agent = OpenAIAgent(**agent_config)
            
            # Execute with RunContextWrapper
            start_time = time.time()
            with RunContextWrapper[OrchestrationContext](orchestration_context) as context:
                # Update orchestration state during execution
                context.orchestration_state["execution_phase"] = "running"
                run_result = await Runner.run(agent, str(task.model_dump()), max_turns=5)
                context.orchestration_state["execution_phase"] = "completed"
                
            execution_time = time.time() - start_time
            
            if not run_result.final_output:
                raise ValueError("No output from agent")
            
            # Use RunResult.new_items instead of manual parsing
            result_content = str(run_result.final_output)
            structured_content = None
            summary = "Task completed"
            
            # Process new_items from RunResult for enhanced output
            if hasattr(run_result, 'new_items') and run_result.new_items:
                # Extract structured data from new_items
                items_data = []
                for item in run_result.new_items:
                    if hasattr(item, 'content'):
                        items_data.append(str(item.content))
                    elif hasattr(item, 'data'):
                        items_data.append(str(item.data))
                    else:
                        items_data.append(str(item))
                
                if items_data:
                    structured_content = json.dumps({
                        "items": items_data,
                        "item_count": len(items_data),
                        "execution_context": {
                            "agent_role": self.agent_data.role,
                            "task_name": task.name
                        }
                    }, indent=2)
                    summary = f"Task completed with {len(items_data)} output items"
            
            # Fallback to JSON parsing if needed
            try:
                result_data = json.loads(result_content)
                if isinstance(result_data, dict):
                    result_content = result_data.get("result", result_content)
                    summary = result_data.get("summary", summary)
                    if not structured_content:
                        structured_content = json.dumps(result_data, indent=2)
            except:
                # Keep original content if not JSON
                pass
            
            # Ensure result_content is always a string
            if isinstance(result_content, (dict, list)):
                result_content = json.dumps(result_content, indent=2)
            elif not isinstance(result_content, str):
                result_content = str(result_content)
                
            output = EnhancedTaskExecutionOutput(
                task_id=task.id,
                status=TaskStatus.COMPLETED,
                result=result_content,
                execution_time=execution_time,
                summary=summary,
                structured_content=structured_content
            )
            
            # PILLAR 6: Save to memory system
            await self._save_to_memory(task, output)
            
            # PILLAR 8: Quality validation
            await self._validate_quality(task, output)
            
            logger.info(f"✅ Task {task.id} completed in {execution_time:.2f}s")
            return output
            
        except Exception as e:
            logger.error(f"❌ Task {task.id} failed: {e}")
            
            # Save failure to memory
            if MEMORY_AVAILABLE:
                await self._save_failure_lesson(task, str(e))
            
            return EnhancedTaskExecutionOutput(
                task_id=task.id,
                status=TaskStatus.FAILED,
                error_message=str(e),
                execution_time=0,
                summary=f"Task failed: {str(e)}"
            )
        finally:
            try:
                await update_agent_status(str(self.agent_data.id), AgentStatus.AVAILABLE.value)
            except:
                pass
    
    async def _save_to_memory(self, task: Task, output: TaskExecutionOutput):
        """Save execution insights to memory (Pillar 6)"""
        if not MEMORY_AVAILABLE:
            return
            
        try:
            # Extract insight based on result
            insight_type = "success_pattern" if output.status == TaskStatus.COMPLETED else "failure_lesson"
            
            # Use correct API parameters
            await unified_memory_engine.store_insight(
                workspace_id=str(task.workspace_id),
                insight_type=insight_type,
                content=output.summary or 'Task processed',
                relevance_tags=[self.agent_data.role, task.name],
                metadata={
                    "task_name": task.name,
                    "agent_role": self.agent_data.role,
                    "execution_time": output.execution_time
                }
            )
            
            logger.info(f"💾 Saved {insight_type} to memory")
            
        except Exception as e:
            logger.warning(f"Failed to save to memory: {e}")
    
    async def _validate_quality(self, task: Task, output: TaskExecutionOutput):
        """Validate output quality (Pillar 8)"""
        if not QUALITY_AVAILABLE or output.status != TaskStatus.COMPLETED:
            return
            
        try:
            # Use correct API - validate_asset_quality instead of validate_output
            validation_result = await unified_quality_engine.validate_asset_quality(
                asset_content=output.result,
                asset_type="task_output",
                workspace_id=str(task.workspace_id),
                domain_context=task.name
            )
            
            if hasattr(validation_result, 'needs_enhancement') and validation_result.needs_enhancement:
                logger.warning(f"⚠️ Output needs quality enhancement: {getattr(validation_result, 'reason', 'Quality check failed')}")
            elif isinstance(validation_result, dict) and validation_result.get("needs_enhancement"):
                logger.warning(f"⚠️ Output needs quality enhancement: {validation_result.get('reason')}")
            else:
                logger.info("✅ Quality validation passed")
                # Quality engine will handle enhancement automatically
                
        except Exception as e:
            logger.warning(f"Quality validation failed: {e}")
    
    async def _save_failure_lesson(self, task: Task, error: str):
        """Save failure lesson to prevent repetition"""
        if not MEMORY_AVAILABLE:
            return
            
        try:
            # Use correct API parameters
            await unified_memory_engine.store_insight(
                workspace_id=str(task.workspace_id),
                insight_type="failure_lesson",
                content=f"Task failed with error: {error}",
                relevance_tags=[self.agent_data.role, "error"],
                metadata={
                    "task_name": task.name,
                    "agent_role": self.agent_data.role,
                    "error": error
                }
            )
            
        except Exception as e:
            logger.warning(f"Failed to save failure lesson: {e}")