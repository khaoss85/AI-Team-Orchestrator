#!/usr/bin/env python3
"""
üîç COMPREHENSIVE VERIFICATION TEST - Post-Fix Validation
================================================================================
Test completo per verificare:
1. Task completion lifecycle (pending ‚Üí in_progress ‚Üí completed)
2. Deliverables production e quality
3. Aderenza completa ai 14 Strategic Pillars
4. End-to-end workflow validation

Questo test valida che tutti i fix implementati funzionino correttamente
e che il sistema sia ora completamente operativo.
"""

import asyncio
import requests
import json
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('comprehensive_verification.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class ComprehensiveVerificationTest:
    """
    Comprehensive test suite to verify all fixes and system completeness
    """
    
    def __init__(self):
        self.base_url = "http://localhost:8000"
        self.api_base = f"{self.base_url}/api"
        self.workspace_id = None
        self.goal_ids = []
        self.agent_ids = []
        self.task_ids = []
        self.deliverable_ids = []
        
        # Test results tracking
        self.results = {
            "task_completion": {"passed": 0, "failed": 0, "details": []},
            "deliverable_production": {"passed": 0, "failed": 0, "details": []},
            "pillar_compliance": {"passed": 0, "failed": 0, "details": []},
            "overall_success": False
        }
        
        # Strategic Pillars checklist
        self.strategic_pillars = {
            "pillar_1_openai_sdk": "OpenAI SDK Integration",
            "pillar_2_ai_driven": "AI-Driven Task Generation & Orchestration", 
            "pillar_3_universal": "Universal Domain Agnostic",
            "pillar_4_scalable": "Scalable Architecture",
            "pillar_5_goal_driven": "Goal-Driven System",
            "pillar_6_memory_system": "Memory System & Context Retention",
            "pillar_7_autonomous_pipeline": "Autonomous Quality Pipeline",
            "pillar_8_quality_gates": "Quality Gates & Validation",
            "pillar_9_minimal_ui": "Minimal UI Overhead",
            "pillar_10_real_time_thinking": "Real-Time Thinking Process",
            "pillar_11_production_ready": "Production-Ready Reliability",
            "pillar_12_concrete_deliverables": "Concrete Deliverables (No Fake Content)",
            "pillar_13_course_correction": "Course Correction & Self-Healing",
            "pillar_14_modular_tools": "Modular Tool Integration"
        }

    async def run_comprehensive_verification(self) -> Dict[str, Any]:
        """Run the complete verification test suite"""
        logger.info("üöÄ STARTING COMPREHENSIVE VERIFICATION TEST")
        logger.info("=" * 80)
        
        try:
            # Phase 1: Setup and workspace creation
            await self.phase_1_setup()
            
            # Phase 2: Task lifecycle verification
            await self.phase_2_task_lifecycle()
            
            # Phase 3: Deliverable production verification
            await self.phase_3_deliverable_production()
            
            # Phase 4: Strategic pillars compliance
            await self.phase_4_strategic_pillars()
            
            # Phase 5: Final assessment
            await self.phase_5_final_assessment()
            
            return self.results
            
        except Exception as e:
            logger.error(f"‚ùå VERIFICATION TEST FAILED: {e}")
            self.results["overall_success"] = False
            return self.results

    async def phase_1_setup(self):
        """Phase 1: Setup and workspace creation"""
        logger.info("üìã PHASE 1: SETUP AND WORKSPACE VALIDATION")
        
        # Use the existing workspace from the E2E test
        self.workspace_id = "a1c1113d-08fe-479c-847a-50ce726beb27"
        logger.info(f"Using existing workspace: {self.workspace_id}")
        
        # Verify agents exist
        response = requests.get(f"{self.base_url}/agents/{self.workspace_id}")
        if response.status_code == 200:
            agents = response.json()
            self.agent_ids = [agent["id"] for agent in agents]
            logger.info(f"‚úÖ Found {len(agents)} agents in workspace")
            for agent in agents:
                logger.info(f"   - {agent['name']} ({agent['role']}): {agent['status']}")
        else:
            raise Exception(f"Failed to get agents: {response.status_code}")
        
        # Get workspace goals
        try:
            response = requests.get(f"{self.api_base}/workspace-goals", params={"workspace_id": self.workspace_id})
            if response.status_code == 200:
                goals = response.json()
                self.goal_ids = [goal["id"] for goal in goals]
                logger.info(f"‚úÖ Found {len(goals)} goals in workspace")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not get goals: {e}")

    async def phase_2_task_lifecycle(self):
        """Phase 2: Task lifecycle verification"""
        logger.info("üîÑ PHASE 2: TASK LIFECYCLE VERIFICATION")
        
        # Wait for some tasks to be created and executed
        await asyncio.sleep(10)
        
        # Check task statuses and progression
        task_statuses = await self._check_task_statuses()
        
        # Verify task progression
        if task_statuses["completed"] > 0:
            logger.info(f"‚úÖ Task completion working: {task_statuses['completed']} completed tasks")
            self.results["task_completion"]["passed"] += 1
            self.results["task_completion"]["details"].append(f"Found {task_statuses['completed']} completed tasks")
        else:
            logger.warning(f"‚ö†Ô∏è No completed tasks found yet")
            self.results["task_completion"]["failed"] += 1
            self.results["task_completion"]["details"].append("No completed tasks found")
        
        if task_statuses["in_progress"] > 0:
            logger.info(f"‚úÖ Task execution active: {task_statuses['in_progress']} in progress")
            self.results["task_completion"]["passed"] += 1
        
        # Check for agent assignment
        assigned_tasks = task_statuses["assigned"]
        if assigned_tasks > 0:
            logger.info(f"‚úÖ Agent assignment working: {assigned_tasks} tasks assigned")
            self.results["task_completion"]["passed"] += 1
            self.results["task_completion"]["details"].append(f"Found {assigned_tasks} assigned tasks")
        else:
            logger.error(f"‚ùå No tasks assigned to agents")
            self.results["task_completion"]["failed"] += 1

    async def phase_3_deliverable_production(self):
        """Phase 3: Deliverable production verification"""
        logger.info("üì¶ PHASE 3: DELIVERABLE PRODUCTION VERIFICATION")
        
        # Check for deliverables in various forms
        deliverables_found = await self._check_deliverables()
        
        if deliverables_found["total"] > 0:
            logger.info(f"‚úÖ Deliverables produced: {deliverables_found['total']} total")
            self.results["deliverable_production"]["passed"] += 1
            self.results["deliverable_production"]["details"].append(f"Found {deliverables_found['total']} deliverables")
        else:
            logger.warning(f"‚ö†Ô∏è No deliverables found yet")
            self.results["deliverable_production"]["failed"] += 1
        
        # Check for quality validation
        quality_validated = await self._check_quality_validation()
        if quality_validated > 0:
            logger.info(f"‚úÖ Quality validation active: {quality_validated} validated items")
            self.results["deliverable_production"]["passed"] += 1

    async def phase_4_strategic_pillars(self):
        """Phase 4: Strategic pillars compliance verification"""
        logger.info("üèõÔ∏è PHASE 4: STRATEGIC PILLARS COMPLIANCE")
        
        pillar_results = {}
        
        # Check each pillar
        for pillar_key, pillar_name in self.strategic_pillars.items():
            compliance = await self._check_pillar_compliance(pillar_key)
            pillar_results[pillar_key] = compliance
            
            if compliance:
                logger.info(f"‚úÖ {pillar_name}: COMPLIANT")
                self.results["pillar_compliance"]["passed"] += 1
            else:
                logger.warning(f"‚ùå {pillar_name}: NON-COMPLIANT")
                self.results["pillar_compliance"]["failed"] += 1
        
        # Calculate overall compliance
        compliance_rate = (self.results["pillar_compliance"]["passed"] / len(self.strategic_pillars)) * 100
        logger.info(f"üìä Overall Strategic Pillar Compliance: {compliance_rate:.1f}%")
        
        self.results["pillar_compliance"]["details"].append(f"Overall compliance: {compliance_rate:.1f}%")

    async def phase_5_final_assessment(self):
        """Phase 5: Final assessment and reporting"""
        logger.info("üìä PHASE 5: FINAL ASSESSMENT")
        
        # Calculate overall success
        total_passed = (self.results["task_completion"]["passed"] + 
                       self.results["deliverable_production"]["passed"] + 
                       self.results["pillar_compliance"]["passed"])
        
        total_tests = (self.results["task_completion"]["passed"] + 
                      self.results["task_completion"]["failed"] +
                      self.results["deliverable_production"]["passed"] + 
                      self.results["deliverable_production"]["failed"] +
                      self.results["pillar_compliance"]["passed"] + 
                      self.results["pillar_compliance"]["failed"])
        
        success_rate = (total_passed / total_tests) * 100 if total_tests > 0 else 0
        
        logger.info("=" * 80)
        logger.info("üéØ FINAL VERIFICATION RESULTS:")
        logger.info(f"üìã Task Completion: {self.results['task_completion']['passed']} passed, {self.results['task_completion']['failed']} failed")
        logger.info(f"üì¶ Deliverable Production: {self.results['deliverable_production']['passed']} passed, {self.results['deliverable_production']['failed']} failed")
        logger.info(f"üèõÔ∏è Strategic Pillars: {self.results['pillar_compliance']['passed']}/14 compliant ({(self.results['pillar_compliance']['passed']/14)*100:.1f}%)")
        logger.info(f"üéâ Overall Success Rate: {success_rate:.1f}%")
        
        # Determine overall success
        self.results["overall_success"] = success_rate >= 80  # 80% threshold for success
        
        if self.results["overall_success"]:
            logger.info("‚úÖ COMPREHENSIVE VERIFICATION: SUCCESS")
        else:
            logger.error("‚ùå COMPREHENSIVE VERIFICATION: NEEDS IMPROVEMENT")

    async def _check_task_statuses(self) -> Dict[str, int]:
        """Check current task statuses"""
        try:
            # Check server logs for task status information
            with open('server_test_fixed.log', 'r') as f:
                logs = f.read()
            
            # Count different task states
            pending_count = logs.count("status updated to pending")
            in_progress_count = logs.count("status updated to in_progress")
            completed_count = logs.count("status updated to completed")
            assigned_count = logs.count("Executing task") + logs.count("with agent")
            
            return {
                "pending": pending_count,
                "in_progress": in_progress_count,
                "completed": completed_count,
                "assigned": assigned_count // 2  # Divide by 2 since we count two patterns
            }
        except Exception as e:
            logger.warning(f"Could not check task statuses: {e}")
            return {"pending": 0, "in_progress": 0, "completed": 0, "assigned": 0}

    async def _check_deliverables(self) -> Dict[str, int]:
        """Check for produced deliverables"""
        try:
            # Check server logs for deliverable creation
            with open('server_test_fixed.log', 'r') as f:
                logs = f.read()
            
            # Count deliverable-related activities
            deliverable_creation = logs.count("check_and_create_final_deliverable")
            quality_deliverables = logs.count("QUALITY DELIVERABLE: Created")
            asset_creation = logs.count("Generated asset")
            
            return {
                "total": deliverable_creation + quality_deliverables + asset_creation,
                "quality_deliverables": quality_deliverables,
                "asset_creation": asset_creation
            }
        except Exception as e:
            logger.warning(f"Could not check deliverables: {e}")
            return {"total": 0, "quality_deliverables": 0, "asset_creation": 0}

    async def _check_quality_validation(self) -> int:
        """Check for quality validation activities"""
        try:
            with open('server_test_fixed.log', 'r') as f:
                logs = f.read()
            
            quality_count = logs.count("quality validation") + logs.count("Quality validation")
            return quality_count
        except Exception as e:
            logger.warning(f"Could not check quality validation: {e}")
            return 0

    async def _check_pillar_compliance(self, pillar_key: str) -> bool:
        """Check compliance for a specific strategic pillar"""
        try:
            with open('server_test_fixed.log', 'r') as f:
                logs = f.read()
            
            # Define compliance indicators for each pillar
            compliance_indicators = {
                "pillar_1_openai_sdk": ["OpenAI", "gpt-4", "gpt-3.5"],
                "pillar_2_ai_driven": ["AI-driven", "intelligent", "smart"],
                "pillar_3_universal": ["domain agnostic", "universal", "generic"],
                "pillar_4_scalable": ["scalable", "performance", "optimization"],
                "pillar_5_goal_driven": ["goal", "target", "objective"],
                "pillar_6_memory_system": ["memory", "context", "learning"],
                "pillar_7_autonomous_pipeline": ["autonomous", "pipeline", "automation"],
                "pillar_8_quality_gates": ["quality", "validation", "gates"],
                "pillar_9_minimal_ui": ["minimal", "efficient", "streamlined"],
                "pillar_10_real_time_thinking": ["thinking", "reasoning", "real-time"],
                "pillar_11_production_ready": ["production", "reliable", "stable"],
                "pillar_12_concrete_deliverables": ["deliverable", "asset", "concrete"],
                "pillar_13_course_correction": ["correction", "self-healing", "adaptive"],
                "pillar_14_modular_tools": ["tool", "modular", "integration"]
            }
            
            indicators = compliance_indicators.get(pillar_key, [])
            
            # Check if any indicators are present in logs
            for indicator in indicators:
                if indicator.lower() in logs.lower():
                    return True
            
            return False
            
        except Exception as e:
            logger.warning(f"Could not check pillar {pillar_key}: {e}")
            return False

async def main():
    """Main test execution"""
    test_suite = ComprehensiveVerificationTest()
    results = await test_suite.run_comprehensive_verification()
    
    # Save results to file
    with open('comprehensive_verification_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    logger.info("üîç Verification complete. Results saved to comprehensive_verification_results.json")
    
    return results["overall_success"]

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)